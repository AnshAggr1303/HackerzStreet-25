# ðŸš€ Multilingual Health RAG System (HackerzStreet 2025)

## ðŸ‘¨â€ðŸ’» Authors
- **Ansh Agrawal, Ekaansh Sawaria , Devansh Sharma , Arnav bansal**
- ðŸ“… Date: **13/4/2025**

Link to the video demonstration: https://drive.google.com/drive/folders/10kBcKm-Aj0kcGC5THhuLprZ6pzduNnxA?usp=share_link



## ðŸ“ Introduction
This project presents a fully **open-source**, **cloud-agnostic** multilingual **Retrieval-Augmented Generation (RAG)** system for **insurance documentation**. Designed to support **multiple Indian languages**, the system enables both **text and voice-based interactions** while ensuring **privacy-friendly, cost-effective deployment** without reliance on proprietary cloud platforms.

By integrating **open-source LLMs (LLaMA)** and leveraging **FastAPI, FastText, Deep Translate**, the system delivers **context-aware responses with enhanced accuracy**.

This assistant not only facilitates **real-time translation** and **multilingual** voice conversations, but also delivers medically-grounded answers using a retrieval-based approach, making it a practical, accessible, and scalable solution for rural and underserved healthcare ecosystems. ðŸ¦ðŸ’¬

### ðŸŒŸ Key Features
- ðŸŒ **Multilingual support** for **9 Indian languages**
- ðŸŽ™ï¸ **Voice and text input processing**
- ðŸ“„ **PDF document processing** and embedding generation
- ðŸ§  **Context-aware responses using RAG**
- ðŸ”„ **Real-time translation**

### ðŸ› ï¸ Tech Stack
- ðŸš€ **FastAPI** for backend
- ðŸ·ï¸ **FastText** for embeddings
- ðŸŒ **Deep Translate** for language translation
- ðŸ¦™ **LLaMA 3 (via Ollama)** for response generation
- âš›ï¸ **React** for frontend

## âš™ï¸ Installation & Setup

### ðŸ“Œ Prerequisites
- ðŸ **Python 3.13+**
- ðŸ“‚ **FastText pre-trained model (cc.en.300.bin)**
- ðŸ’» **Node.js and npm for frontend**
- ðŸ¦™ **Ollama installed and running**

### ðŸ“¥ Installation Steps
1ï¸âƒ£ Clone the repository:
```bash
git clone https://github.com/AnshAggr1303/HackerzStreet-25.git
cd HackerzStreet-25
```

2ï¸âƒ£ Create and activate virtual environment:
```bash
python -m venv rag_env
source rag_env/bin/activate  # On Windows: rag_env\Scripts\activate
```

3ï¸âƒ£ Install dependencies:
```bash
cd backend
pip install -r requirements
```

## ðŸš€ Usage Guide

### ðŸ¦™ Running the Ollama server (open-source LLM)
ðŸ“¥ Download Ollama from [Ollama Official Site](https://ollama.com/download)

Run the following commands in terminal:
```bash
ollama serve
ollama run llama3
```
To stop the server:
```bash
ollama stop llama3
```

### ðŸ¦™ Downloading the embeddings model (Fasttext by Facebook Research)
Run the following commands in terminal in the backend/app/models folder:
```bash
wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz
gunzip cc.en.300.bin.gz
```


### â–¶ï¸ Running the Project (after creating your environment)

1ï¸âƒ£ Start the FastAPI server:
```bash
cd backend
uvicorn app.main:app --reload
```

2ï¸âƒ£ Start the frontend server:
```bash
cd frontend
npm i
npm run dev
```

## ðŸ›ï¸ Architecture Overview

### ðŸ“‚ Directory Structure
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ translation_service.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â””â”€â”€ speech_service.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ embeddings_output/
â””â”€â”€ models/
```

### ðŸ”‘ Core Components
ðŸ“„ **Embedding Service** : Processes PDFs and generates FastText embeddings  
ðŸ” **RAG Service** : Handles context retrieval and query processing  
ðŸŒŽ **Translation Service** : Manages language detection and translation  
ðŸŽ¤ **Speech Service** : Handles voice input/output processing  
ðŸ§  **LLM Service** : Interfaces with LLaMA for response generation  

## ðŸ“¡ API Documentation

### ðŸ”Œ Endpoints

#### ðŸ“¤ `POST /api/upload`
Handles **voice input processing** ðŸŽ¤
```json
Request:
- FormData with 'file' field (audio/mpeg)

Response:
{
    "audio_content": bytes,
    "detected_language": string
}
```

#### ðŸ’¬ `POST /api/query`
Handles **text input processing** ðŸ“
```json
Request:
{
    "prompt": string
}

Response:
{
    "response": string,
    "detected_language": string
}
```

### ðŸ“š References
- ðŸ“˜ [FastAPI Documentation](https://fastapi.tiangolo.com/)
- ðŸ“— [FastText Documentation](https://fasttext.cc/)
- ðŸ¦™ [LLaMA Documentation](https://github.com/facebookresearch/llama)

# ðŸ“ Notes

- Currently working on adding **support for other languages** in both:
  - **Speech-to-Text (STT)**
  - **Text-to-Speech (TTS)**

ðŸŽ¥ **Backend Implementation of Multilingual Capabilities Walkthrough**  
[Click here to watch the video](https://drive.google.com/drive/folders/1fNfXkMAyTccvzM_NgL20x1SrRz_M6UmH?usp=share_link)

---

> More updates coming soon as we integrate multilingual Speech capabilities!
